### Introduction
***
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. 
We are required to create a Postgres database with tables designed to optimize queries on song play analysis. 
We are required to –
* Create a database schema
* ETL pipeline for Analysis
* Test the database and ETL pipeline by running queries

### Project Description
*** 
Input data is in the form of 2 datasets –

**Song Dataset** – This dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
Through this dataset we will be extracting 2 dimensional tables called as users and songs.

**Log Dataset** – This dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
Through this dataset we will be extracting 2 dimensional tables called as artists and time.
All the 4 tables extracted through 2 datasets will be connected to one Fact Table called as songplays as mentioned below in the Star Schema.

<img src="star_schema.JPG" alt="drawing" width="500"/>

#### Users table Columns –
user_id, first_name, last_name, gender, level

#### songs table columns –
song_id, title, artist_id, year, duration

#### artists table columns –
artist_id, name, location, latitude, longitude

#### time table columns –
start_time, hour, day, week, month, year, weekday

### Project Steps –
* Drop tables, Create tables and Insert tables queries were filled in **sql_queries.py**. 
* **create_tables.py** has defined functions to create connection with postgres database, create tables and drop tables
* Through **etl.ipynb** we tested the steps one by one as required, which we need to code in etl.py for final execution of data extraction through all the files in provided filepath.
* All the actions over tables were tested in dedicated Jupyter notebook called as **test.ipynb**
* Finally coding was done in **etl.py** and was executed through terminal to process all data in all the files

### Conclusion
All steps were executed and project rubric mentioned were achieved to produce the required analysis for this project.